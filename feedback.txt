Assumptions & Design Considerations
===================================================================================================================
 1. Required to perform with minimal library installations.
 2. Even with making 10% of Data hits, the data from Database is not cached as its assumed low memory system.
 3. As opposed the point 2, the solution is built in a way to use a cache=True flag in (db_handler) so it caches
    database response. so it doesn't have to go via network call everytime.
 4. mysql-connector-python has been used and reference of below has been used while coding
     - https://dev.mysql.com/doc/connector-python/en/
 5. matches are performed on all fields first name, last name, location. As its sensible to do so to correct identify
    right user.
 6. the query is included in the db_handler file with respect of time, but it should be configured as config file,
    to separate of concerns for code and configuration.
 7. Since our goal is data warehouse, its always necessary to hold the state changes in the data, so audit fields are
    mandatory to include. Hence, added it into the tables.
 8. REST calls are multi-threaded to save sometime from sequential calls. Its assumed there is no restrictions to
    make calls concurrently within 10% of data.

Files of Interest
===================================================================================================================
- feedback.txt - Assumptions & Design consideration (also the current file you are reading)
- requrements.txt - Python solution dependencies.
- rest_connections.txt - to hold URL configuration
- db_connections.txt - DB connection params (the password is used in clear letters, with more time, we can encrypt it)
- instructions.txt - provides instructions to the execution of the code.
- sql_ddl.txt - Data definition statement for reference. ( not performed syntax checks).
- sql_dml.txt - generated by code. ( not performed syntax checks).

Alternative Solutions.
===================================================================================================================
 1. Pandas Dataframe can be used to load the data returned from API and Database and then can be performed
    a join condition to identify matching data.
 2. Spark Dataframe can be utilized making it Big Data based solutions similarly join conditions can be performed
    there.